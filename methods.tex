\section{Methods} \label{sec: Methods}

\subsection{Mechanical model} \label{subsec: Mechanical model}

The system under investigation is a multi-link inverted pendulum on a cart. This setup, known for its inherent instability and dynamic nature, is a cornerstone in control theory~\cite{Fantoni2001Nonlinear}. The cart, serving as the base platform for the pendulum links, is limited to linear motion along a horizontal track, simplifying the translational dynamics to one-dimensional motion along the x-axis. The cart's movement is controlled by an externally applied force $F$, which is crucial for the system's stabilization. The magnitude and direction of this force significantly impact the system's dynamics, providing a means to counteract the gravitational torque imposed by the pendulum links. The details of the system modelling are described in~\cite{manzl2023relrl}.

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{Figures/cart_pole_model_phi.pdf}
\caption{$N$-link inverted pendulum on a cart. Minimal coordinates formulation $\mathbf{q} = \mathbf{s} =  [x_{\text{cart}}, \varphi_1, \ldots, \varphi_N]$ are used to model the system. Each link consists of length $l_j$, mass $m_j$, moment of inertia $I_j$. The cart is a rigid body with mass $m_{cart}$.}
\label{fig: n-pendulum on a cart}
\end{figure}

Attached to the cart is a series of n rigid links, each connected end-to-end by revolute joints. These joints allow for free rotational movement in the vertical plane, and each link 
$j$ is characterized by mass $m_j$, length $l_j$ and inertia $I_j$.
This model, with its high degree of control difficulty and relevance to real-world applications, provides a valuable platform for testing sophisticated control algorithms, including those based on Reinforcement Learning.

\subsection{Reinforcement Learning with continuous action space} \label{subsec: Reinforcement Learning with continuous action space}
Reinforcement Learning is a branch of machine learning where an agent learns optimal behavior through systematic interaction with a dynamic environment, aiming to maximize cumulative rewards. This learning paradigm is distinct from supervised learning; in RL, the agent is not explicitly instructed which actions to take. Instead, it must explore and discover which actions yield the highest rewards by trial and error, a process often facilitated by a policy—a decision-making function that maps states of the environment to actions to be taken in those states~\cite{sutton_reinforcement_2018}.

In our research, RL is employed to train agents to manage the dynamics of a multi-link inverted pendulum on a cart, a challenging control problem that requires maintaining precise dynamic stability throughout the process. The reward function in RL plays a crucial role as it guides the learning process. For instance, a reward function can be designed to penalize the agent for excessive movement away from a target state or for using too much energy, while rewarding closer approximations of the desired state, such as maintaining the pendulum in an upright position. We use the reward function, which has shown one of the best performances from our previous study
\begin{equation}
r = 1 - w_p \frac{\left|p_x\right|}{\chi_\mathrm{cart}} - (1-w_p) \frac{\sum_{j=1}^\mathrm{N} \left|\varphi_j\right|}{\mathrm{N} \chi_{\varphi}} 
\label{eq:reward}
\end{equation}
It combines a tip position with the sum of pendulum link angles $\sum_{j=1}^{N} \left|\varphi_j\right|$, weighted by a factor $w_p$. $\chi_\mathrm{cart}$ and $\chi_{\varphi}$ are predefined, constant position and angle thresholds, which differ from 1-link to more link systems. $\mathrm{N}$ is the number of the links.

In RL, the choice between discrete and continuous action space might significantly affect the performance of learning algorithms. Discrete action space, used in our previous study, limits the agent’s actions to a finite set of possibilities: it either applies a negative or a positive force of the same magnitude to regulate the behavior of the system. In a continuous action space the agent is provided with an interval of the control force
\begin{equation}
F = [-f_\mathrm{cart}, +f_\mathrm{cart}]
\label{eq:force}
\end{equation}
From this interval in Eq.~\eqref{eq:force} the agent is free to choose any suitable control force as an action for learning the stabilization task. Since it provides more possible actions then the discrete action space, it emerges in a faster training of an agent and achieves a more smooth and robust control task execution.  
% NOTE: 1) continue with - adding a final punchline to this section
% While simpler to implement, this can restrict the agent's ability to finely tune its responses to the environment's demands.

\subsection{Model evaluation} \label{subsec: Model evaluation}
\todo[inline, disable]{Write a single paragraph explaining the model evaluation in simple words. - addressed with the first paragraph}
The evaluation of the model follows the same methodology used in our previous research by Manzl et al.~\cite{manzl2023relrl}. The agent's performance is periodically assessed using two primary metrics: the mean reward must exceed a defined threshold \( \lambda_r \), and the loss must remain below a threshold \( \lambda_l \). These thresholds serve as indicators of the agent's learning progress and effectiveness in the task. \todo[inline, disable]{This statement is inaccurate. Is it really the base for evaluation? - what's the inaccuracy here? I have no clue how does the tests framework operates "under the hood" - its function is inside exudyn framework especially for machine learning evaluation and the explanation inside of it is just bad | here at the beginning i've made just a simple copy-paste from the first paper because even there if you're reading it you can't really understand that evaluation pipeline properly}
During each evaluation phase, the agent undergoes \( n_{\text{test}} \) tests in a simulated environment over \( n_{\text{eval}} \) time steps, where \( n_{\text{eval}} \) is greater than \( n_{\text{learn}} \). This inequality, \( n_{\text{eval}} > n_{\text{learn}} \), ensures that the evaluation period allows for more comprehensive testing of the model's generalization beyond the learning phase.\todo[inline, disable]{Write what does this inequality means - done in the previous sentence}
For example, with a timestep of \( h = 0.02 \) seconds, setting \( n_{\text{eval}} = 5000 \) results in a 100-second evaluation duration.
To introduce variability in the tests, the agent's initial states are perturbed within a range of \( \pm x_{\text{init}} \), and the maximum norm of the state vector is recorded at each time step \( t_i \) to compute the test error \( e_{\text{test},i} = \|\mathbf{s}_i\|_{\infty} \). The overall test error \( e_{\text{test}} \) is defined as the maximum error encountered during the final quarter of the evaluation period:
\begin{equation}
	e_{\text{test}} = \max(e_{\text{test},i}) \quad \forall \, i \geq \frac{3}{4}n_{\text{eval}}.
\end{equation}
The training is considered complete when the test error \( e_{\text{test}} \) falls below a predefined threshold \( \chi_{\text{test}} \) across all tests, offering a potential early stopping criterion.
To assess robustness, several agents are trained and evaluated, and their performance is visualized through error envelopes that display the best, worst, and mean error metrics over time. It should be noted that early evaluation results may be unreliable, as the model may not have fully converged and therefore the evaluation is performed after the model has gone through certain training timesteps.\todo[inline, disable]{Does the same value is valid for each $n$-pendulum? - better was to delete the statement of 50000, or maybe it's even better to delete the remaining text starting from "It should be..."}

\subsection{Performance evaluation of PPO with continuous action space} \label{subsec: Performance evaluation of PPO with continuous action space}

In this study, we evaluate the performance of the PPO Reinforcement Learning algorithm using its continuous action space version, building upon our previous work where only a discrete action space was employed for RL-training.\todo[inline, disable]{What is the 'evaluation of an action space benefits'. Make it clearer. - RE-WRITTEN} PPO is an algorithm designed to improve the stability and efficiency of policy gradient methods by optimizing a surrogate objective function.\todo[inline, disable]{This note is unclear without any reference to what 'policy gradient methods' are any why they need improvements. It is needed?}\todo[inline, disable]{This is even more confusing. Rewrite. - addressed in the text now about policy gradient methods and surrogate objective function | we can delete it and point out that we just use PPO.}
Traditional policy gradient methods can suffer from instability when large updates to the policy are made, potentially leading to poor performance. PPO addresses this by introducing a clipped objective function that limits the extent of policy updates, ensuring more stable and consistent training~\cite{schulman2017ppo}.
Our evaluation focuses on an $N$-link inverted pendulum system, where we assess performance with 1-link, 2-link, and 3-link configurations. The primary goal is to analyze how the use of a continuous action space affects the agent's ability to stabilize the pendulum at the upward-facing equilibrium, in comparison to the discrete action space used in earlier research. The task becomes progressively more challenging with the addition of links, as each additional link increases the instability and complexity of the system. While the single-link pendulum is a classic benchmark for reinforcement learning methods, this study extends the evaluation to more complex systems with multiple links, providing a broader perspective on the effectiveness of continuous action spaces.
The experiments are conducted using Exudyn Version 1.8.52 and stable-baselines3 Version 1.8.0. Each experiment is repeated 10 times with different random seeds to ensure robustness against variations in initialization. The PPO algorithm is used with standard parameters, except for specific adjustments made to accommodate the continuous action space and the particular requirements of the environment.\todo[inline, disable]{Used? I do not think we have implemented it. - clarified, not implemented but used}
Table~\ref{tab:hyperparameters} summarizes the hyperparameters used in the experiments, and Table~\ref{tab:env_params} outlines the environment, reward, and training parameters for the different link configurations. Notably, the cart force is now continuous and is presented as an interval, ranging from \([-12, 12]\) N for the 1-link system, \([-40, 40]\) N for the 2-link and \([-60, 60]\) N for the 3-link system.\todo[inline, disable]{Show numbers for all cases or none of them. - added numbers for other links}

\begin{table}[h]
	\centering
	\caption{The hyperparameters used for the PPO method in the experiments. The physical parameters are provided in Table~\ref{tab:env_params}.}
	\label{tab:hyperparameters}
	\begin{tabular}{ll|ll}
		\toprule
		\textbf{Parameter}       & \textbf{Value} & \textbf{Parameter}       & \textbf{Value} \\ \midrule
		Reward function          & $r$, Eq.~\ref{eq:reward},  & Reward threshold         & $\lambda_r = 0.9$ \\ 
		Step size                & 20 ms           & Loss threshold           & $\lambda_l = 0.01$ \\ 
		Evaluation length        & 5000 steps $\Rightarrow$ 100 s & PPO: $n_{\text{steps}}$       & $n_{\text{episode,max}}$ \\ 
		Learning rate      & $5 \cdot 10^{-4}$ & & \\ \bottomrule
	\end{tabular}
\end{table}

\begin{table}[h]
	\centering
	\caption{Environmental, reward, and training parameters for the environments with link numbers 1 to 3.}
	\label{tab:env_params}
	\begin{tabular}{l l c c c}
		\toprule
		\textbf{Name} & \textbf{Parameter} & \textbf{1 link} & \textbf{2 link} & \textbf{3 link} \\ \midrule
		Cart force              & $f_{\text{cart}}$ in N         & \([-12, 12]\)   & \([-40, 40]\)   & \([-60, 60]\)  \\ 
		Threshold cart position & $\chi_x$ in m                 & 1.2  & 3.6  & 5.4 \\ 
		Threshold link angle    & $\chi_\varphi$ in rad         & $\frac{\pi}{20}$ & $\frac{\pi}{10}$ & $\frac{3\pi}{20}$ \\
%		For $\varphi_1$ to $\varphi_N$ \todo[inline, size=\tiny, inlinewidth=3cm]{Is this needed here? Confusing. JUST COMMENTED OUT THIS LINE}  & & & & \\ 
		Max test error          & $e_{\text{test}}$             & 0.2  & 0.5  & 0.75 \\ 
		Reward position factor  & $w_p$                         & 0.5  & 0.5  & 0.8 to 1 \\ 
		Required training steps & $n_{\text{learn}}$            & $80 \cdot 10^3$ & $150 \cdot 10^3$ & $350 \cdot 10^3$ \\ 
		Max episode length      & $n_{\text{episode,max}}$      & 1280 & 1536 & 2048 \\ 
		Tests per evaluation    & $n_{\text{test}}$             & 50   & 50   & 70 \\ 
		\bottomrule
	\end{tabular}
\end{table}

The continuous action space, particularly in the control of the cart force, provides more nuanced control via the specified interval of the force, which is expected to influence the stabilization process, especially in the more complex multi-link systems.

\subsection{Curriculum learning implementation} \label{subsec: Curriculum learning implementation}
Our approach to Curriculum Learning in the domain of Multibody System Dynamics is based on the gradual introduction of complexity to the RL agent’s learning environment. As outlined in the taxonomy by Narvekar et al.~\cite{narvekar2020survey}, this approach is grounded in Transfer Learning, where simpler tasks are used to build the foundation for more complex ones.
In this study, Curriculum Learning is physically implemented through the use of spring-damper elements, which play a crucial role in controlling the system's dynamic behavior.\todo[inline, disable]{The second part of this sentence is unclear. What plays a crucial role and what does 'regulating the dynamic behavior' means in our context? - clarified}. These elements are strategically positioned between various system components to manage the stability and interactions throughout the learning process. A schematic of this physical implementation is provided in Figure~\ref{fig: cl mechanical implementation}. We utilize two types of spring-damper systems: translational and rotational.
The translational spring-damper is placed between the cart body and the ground, and its primary function is to limit the cart’s translational movement along the x-axis. By moderating this movement, the translational spring-damper effectively controls the pendulum’s speed, ensuring greater stability during the initial phases of learning.\todo[inline, disable]{Make this statement more straighforward and clear. - done} This stability is essential for the agent to explore the system dynamics without being destabilized by excessive movement. The rotational spring-damper systems are attached to the revolute joints, regulating the angular displacement of the pendulum. These systems provide resistance to angular changes, thus preventing abrupt rotational movements that could destabilize the learning process. By controlling the angular behavior, the rotational spring-dampers ensure that the pendulum remains within a manageable range of motion during the early stages of learning.

\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{Figures/cl_mech_implementation_v1.pdf}
	\caption{Scheme of the system with translational and rotational spring-dampers. \todo[inline, disable]{Make it consistent with Figure 1. - it is consistent with the mechanical system figure: what is the problem?}}
	\label{fig: cl mechanical implementation}
\end{figure}

These mechanical components act as stabilizers, simplifying the learning environment and helping the agent adapt to the system's complex dynamics. As training progresses, the influence of the spring-damper systems is gradually reduced - either by lowering their stiffness and damping coefficients or by removing them entirely. This ensures the agent transitions to full control of the system and becomes capable of managing the dynamics independently, without relying on the initial stabilizing aids.\todo[inline, disable]{Totally inaccurate. What is the constraint in multibody dynamics?} \todo[inline, disable]{Rewrite the selected part. Make it shorter, easier to read. - re-written}
Following the mechanical setup involving spring-damper systems described earlier, our CL scheme includes four key parameters outlined in Table~\ref{table: CL parameters}, each controlling a different aspect of how the spring-damper elements influence the system. These parameters define the decay functions (types) that gradually reduce the effect of the spring-dampers, allowing the agent to progressively take more control. The decay functions are parametrized to adjust the stiffness and damping coefficients as training progresses. As the output of the decay function reaches zero, the CL influence diminishes to zero and the RL agent takes a full control of handling the task. For the system under study the control values are represented as a vector, where each element corresponds to the restriction of the system part: the first value describes the cart translational movement restriction, while the next values describe the angular movement restriction of the pendulum links.\todo[inline, disable]{What physical setup you are referring here?} \todo[inline, disable]{Description of those parameters is unclear as the setup is not defined. You must include decay functions and show how they are parametrized. - IMPROVED, but what do you actually mean by parametrized? how should i include decay functions? the description about them is provided in the table as well as the plot of their behavior in our study}

\begin{table}[h]
	\caption{CL parameters and their descriptions}
	\centering
	\begin{tabular}{l|p{0.7\textwidth}}
		\toprule
		\makecell{\textbf{Curriculum}\\ \textbf{Learning}\\ \textbf{parameters}} & \textbf{Description} \\ \midrule
		Control values & control parameters representing spring-damper values, which influence the restriction of the system behavior\\ \hline 
		Decay steps & time steps when the transition to another set of control values occurs\\ \hline 
		Decay function & describes the law on how the control values will be changed\\ \hline
		Decay factor & sets the speed of decay function\\ 
		\bottomrule
	\end{tabular}
	\label{table: CL parameters}
\end{table}

Our approach incorporates predefined decay functions that govern the rate and pattern of assistance withdrawal, ensuring a smooth transition of control from the spring-damper systems to the agent. These decay functions define how the stiffness and damping coefficients of the spring-dampers decrease over time, gradually reducing their influence. In this research we have attempted to utilize five distinct decay types: Linear, Exponential, Quadratic, Square Root, and Discrete.\todo[inline, disable]{Those functions require more detailed description. - addressed here and after}

\begin{figure}[ht]
	\centering
	\includegraphics[width=13cm]{Figures/CL_decay_types_comparison.png}
	\caption{Decay functions comparison for a given set of control values [10, 0]. At the decay step equal to 5 all the functions take the next control value of 0.}
	\label{fig: decay functions}
\end{figure}

Each decay function follows a different trajectory, as demonstrated in the Figure~\ref{fig: decay functions}:

\begin{itemize}
	\item \textbf{Linear:} decreases at a constant rate over time.
	\item \textbf{Exponential:} decays rapidly at first and then gradually slows down as it approaches zero.
	\item \textbf{Quadratic:} starts slowly but accelerates as time progresses.
	\item \textbf{Square Root:} faster at the beginning but slows down towards the end.
	\item \textbf{Discrete:} remains constant for most of the time and then drops sharply to zero at the end of the decay period.
\end{itemize}

In general, a decay function could be any function which could take the control values as an input and respond with its decreased values over certain training time. Selection of a decay function can be handled with trial and error and understanding the pattern of how the CL control must be diminished throughout the training period.